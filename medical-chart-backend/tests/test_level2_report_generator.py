"""
Level 2 çµ±åˆãƒ†ã‚¹ãƒˆ: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«: Level 2 (Integration)
ä¿¡é ¼æ€§: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ 95%
å®Ÿè¡Œé »åº¦: é€±æ¬¡
å‰ææ¡ä»¶: Level 2çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¾Œ
è¨­è¨ˆæ›¸å‚ç…§: doc_04_detailed_design.md
æ›´æ–°æ—¥: 2025-01-15

ä½¿ç”¨æ–¹æ³•:
pytest tests/test_level2_report_generator.py -v -m integration
"""

import json
import os
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from pathlib import Path

import pytest


@dataclass
class Level2TestResult:
    """Level 2çµ±åˆãƒ†ã‚¹ãƒˆçµæœ"""
    test_name: str
    test_class: str
    test_method: str
    status: str  # "passed", "failed", "skipped"
    duration: float
    error_message: Optional[str] = None
    warnings: List[str] = None
    performance_metrics: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []
        if self.performance_metrics is None:
            self.performance_metrics = {}


@dataclass
class Level2TestSummary:
    """Level 2çµ±åˆãƒ†ã‚¹ãƒˆã‚µãƒãƒªãƒ¼"""
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    total_duration: float
    success_rate: float
    test_categories: Dict[str, int]
    performance_summary: Dict[str, Any]
    generated_at: datetime
    
    def __post_init__(self):
        if self.test_categories is None:
            self.test_categories = {}
        if self.performance_summary is None:
            self.performance_summary = {}


class Level2TestReportGenerator:
    """Level 2çµ±åˆãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "test_reports/level2"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.test_results: List[Level2TestResult] = []
        self.start_time = datetime.utcnow()
    
    def add_test_result(self, result: Level2TestResult):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’è¿½åŠ """
        self.test_results.append(result)
    
    def generate_summary(self) -> Level2TestSummary:
        """ãƒ†ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r.status == "passed"])
        failed_tests = len([r for r in self.test_results if r.status == "failed"])
        skipped_tests = len([r for r in self.test_results if r.status == "skipped"])
        
        total_duration = sum(r.duration for r in self.test_results)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªåˆ†æ
        test_categories = {}
        for result in self.test_results:
            category = self._extract_category(result.test_class)
            test_categories[category] = test_categories.get(category, 0) + 1
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
        performance_summary = self._generate_performance_summary()
        
        return Level2TestSummary(
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            skipped_tests=skipped_tests,
            total_duration=total_duration,
            success_rate=success_rate,
            test_categories=test_categories,
            performance_summary=performance_summary,
            generated_at=datetime.utcnow()
        )
    
    def _extract_category(self, test_class: str) -> str:
        """ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º"""
        if "API" in test_class:
            return "APIçµ±åˆ"
        elif "Service" in test_class:
            return "ã‚µãƒ¼ãƒ“ã‚¹çµ±åˆ"
        elif "Database" in test_class:
            return "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ"
        elif "Performance" in test_class:
            return "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"
        else:
            return "ãã®ä»–"
    
    def _generate_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        performance_results = [
            r for r in self.test_results 
            if r.performance_metrics and len(r.performance_metrics) > 0
        ]
        
        if not performance_results:
            return {}
        
        summary = {
            "total_performance_tests": len(performance_results),
            "average_response_time": 0,
            "max_response_time": 0,
            "min_response_time": float('inf'),
            "performance_warnings": []
        }
        
        response_times = []
        for result in performance_results:
            metrics = result.performance_metrics
            if "response_time" in metrics:
                response_times.append(metrics["response_time"])
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Šåé›†
            if result.warnings:
                for warning in result.warnings:
                    if "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹" in warning:
                        summary["performance_warnings"].append({
                            "test": result.test_name,
                            "warning": warning
                        })
        
        if response_times:
            summary["average_response_time"] = sum(response_times) / len(response_times)
            summary["max_response_time"] = max(response_times)
            summary["min_response_time"] = min(response_times)
        
        return summary
    
    def generate_markdown_report(self, summary: Level2TestSummary) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = f"""# Level 2 çµ±åˆãƒ†ã‚¹ãƒˆ ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚µãƒãƒªãƒ¼

- **å®Ÿè¡Œæ—¥æ™‚**: {summary.generated_at.strftime('%Y-%m-%d %H:%M:%S UTC')}
- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {summary.total_tests}
- **æˆåŠŸ**: {summary.passed_tests} âœ…
- **å¤±æ•—**: {summary.failed_tests} âŒ
- **ã‚¹ã‚­ãƒƒãƒ—**: {summary.skipped_tests} â­ï¸
- **æˆåŠŸç‡**: {summary.success_rate:.1f}%
- **ç·å®Ÿè¡Œæ™‚é–“**: {summary.total_duration:.2f}ç§’

## ğŸ“ˆ ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ

| ã‚«ãƒ†ã‚´ãƒª | ãƒ†ã‚¹ãƒˆæ•° |
|---------|---------|
"""
        
        for category, count in summary.test_categories.items():
            report += f"| {category} | {count} |\n"
        
        report += f"""
## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

"""
        
        if summary.performance_summary:
            perf = summary.performance_summary
            report += f"""- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆæ•°**: {perf.get('total_performance_tests', 0)}
- **å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {perf.get('average_response_time', 0):.3f}ç§’
- **æœ€å¤§ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {perf.get('max_response_time', 0):.3f}ç§’
- **æœ€å°ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {perf.get('min_response_time', 0):.3f}ç§’

"""
            
            if perf.get('performance_warnings'):
                report += "### âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Š\n\n"
                for warning in perf['performance_warnings']:
                    report += f"- **{warning['test']}**: {warning['warning']}\n"
                report += "\n"
        else:
            report += "- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\n\n"
        
        # è©³ç´°ãƒ†ã‚¹ãƒˆçµæœ
        report += "## ğŸ“‹ è©³ç´°ãƒ†ã‚¹ãƒˆçµæœ\n\n"
        
        # æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆ
        passed_tests = [r for r in self.test_results if r.status == "passed"]
        if passed_tests:
            report += "### âœ… æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆ\n\n"
            for result in passed_tests:
                report += f"- **{result.test_name}** ({result.duration:.2f}ç§’)\n"
                if result.warnings:
                    for warning in result.warnings:
                        report += f"  - âš ï¸ {warning}\n"
            report += "\n"
        
        # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ
        failed_tests = [r for r in self.test_results if r.status == "failed"]
        if failed_tests:
            report += "### âŒ å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ\n\n"
            for result in failed_tests:
                report += f"- **{result.test_name}** ({result.duration:.2f}ç§’)\n"
                if result.error_message:
                    report += f"  - ã‚¨ãƒ©ãƒ¼: `{result.error_message}`\n"
            report += "\n"
        
        # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆ
        skipped_tests = [r for r in self.test_results if r.status == "skipped"]
        if skipped_tests:
            report += "### â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆ\n\n"
            for result in skipped_tests:
                report += f"- **{result.test_name}**\n"
                if result.error_message:
                    report += f"  - ç†ç”±: {result.error_message}\n"
            report += "\n"
        
        # æ¨å¥¨äº‹é …
        report += self._generate_recommendations(summary)
        
        return report
    
    def _generate_recommendations(self, summary: Level2TestSummary) -> str:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = "## ğŸ’¡ æ¨å¥¨äº‹é …\n\n"
        
        # æˆåŠŸç‡ã«åŸºã¥ãæ¨å¥¨äº‹é …
        if summary.success_rate < 80:
            recommendations += "- âš ï¸ **æˆåŠŸç‡ãŒ80%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™**ã€‚å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®åŸå› ã‚’èª¿æŸ»ã—ã€ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚\n"
        elif summary.success_rate < 95:
            recommendations += "- ğŸ“ˆ æˆåŠŸç‡ã¯è‰¯å¥½ã§ã™ãŒã€ã•ã‚‰ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚\n"
        else:
            recommendations += "- âœ… å„ªç§€ãªæˆåŠŸç‡ã§ã™ã€‚ç¾åœ¨ã®å“è³ªã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚\n"
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŸºã¥ãæ¨å¥¨äº‹é …
        if summary.performance_summary:
            perf = summary.performance_summary
            avg_time = perf.get('average_response_time', 0)
            
            if avg_time > 2.0:
                recommendations += "- ğŸŒ **å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒ2ç§’ã‚’è¶…ãˆã¦ã„ã¾ã™**ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n"
            elif avg_time > 1.0:
                recommendations += "- â±ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã«æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚\n"
            
            if perf.get('performance_warnings'):
                recommendations += "- âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘ŠãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã«åŸºã¥ãæ¨å¥¨äº‹é …
        if summary.total_tests < 10:
            recommendations += "- ğŸ“Š **ãƒ†ã‚¹ãƒˆæ•°ãŒå°‘ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™**ã€‚è¿½åŠ ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n"
        
        # å¤±æ•—ãƒ†ã‚¹ãƒˆã«åŸºã¥ãæ¨å¥¨äº‹é …
        if summary.failed_tests > 0:
            recommendations += f"- ğŸ”§ {summary.failed_tests}ä»¶ã®å¤±æ•—ãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã™ã€‚å„ªå…ˆçš„ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚\n"
        
        recommendations += "\n"
        return recommendations
    
    def save_json_report(self, summary: Level2TestSummary, filename: str = None) -> str:
        """JSONãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        if filename is None:
            timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            filename = f"level2_integration_test_report_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        report_data = {
            "summary": asdict(summary),
            "test_results": [asdict(result) for result in self.test_results],
            "metadata": {
                "report_version": "1.0",
                "generator": "Level2TestReportGenerator",
                "test_level": "Level 2 (Integration)"
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
        
        return str(filepath)
    
    def save_markdown_report(self, summary: Level2TestSummary, filename: str = None) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        if filename is None:
            timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            filename = f"level2_integration_test_report_{timestamp}.md"
        
        filepath = self.output_dir / filename
        
        markdown_content = self.generate_markdown_report(summary)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return str(filepath)


class TestLevel2ReportGeneration:
    """
    Level 2 çµ±åˆãƒ†ã‚¹ãƒˆ: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    """
    
    @pytest.mark.integration
    def test_level2_report_generator_functionality(self):
        """
        ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«: Level 2 (Integration)
        å¯¾è±¡: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½çµ±åˆãƒ†ã‚¹ãƒˆ
        """
        print("\nğŸ§ª Level 2çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½")
        
        # Step 1: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨åˆæœŸåŒ–
        print("ğŸ“‹ Step 1: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨åˆæœŸåŒ–")
        generator = Level2TestReportGenerator("test_reports/level2_test")
        
        # Step 2: ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆçµæœè¿½åŠ 
        print("ğŸ“Š Step 2: ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆçµæœè¿½åŠ ")
        
        # æˆåŠŸãƒ†ã‚¹ãƒˆçµæœ
        generator.add_test_result(Level2TestResult(
            test_name="ãƒãƒ£ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰APIçµ±åˆ",
            test_class="TestLevel2ChartAPIIntegration",
            test_method="test_chart_upload_api_integration",
            status="passed",
            duration=2.5,
            performance_metrics={"response_time": 1.2, "throughput": 10}
        ))
        
        generator.add_test_result(Level2TestResult(
            test_name="ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆCRUDçµ±åˆ",
            test_class="TestLevel2TemplateServiceIntegration",
            test_method="test_template_crud_integration",
            status="passed",
            duration=1.8,
            warnings=["DBæ¥ç¶šãŒä¸€æ™‚çš„ã«é…å»¶ã—ã¾ã—ãŸ"]
        ))
        
        # å¤±æ•—ãƒ†ã‚¹ãƒˆçµæœ
        generator.add_test_result(Level2TestResult(
            test_name="GCSçµ±åˆãƒ†ã‚¹ãƒˆ",
            test_class="TestLevel2GCSDBServiceIntegration",
            test_method="test_gcs_integration",
            status="failed",
            duration=0.5,
            error_message="GCSèªè¨¼ã‚¨ãƒ©ãƒ¼"
        ))
        
        # ã‚¹ã‚­ãƒƒãƒ—ãƒ†ã‚¹ãƒˆçµæœ
        generator.add_test_result(Level2TestResult(
            test_name="å¤–éƒ¨APIçµ±åˆãƒ†ã‚¹ãƒˆ",
            test_class="TestLevel2ExternalAPIIntegration",
            test_method="test_external_api_integration",
            status="skipped",
            duration=0.0,
            error_message="ãƒ†ã‚¹ãƒˆç’°å¢ƒåˆ¶é™"
        ))
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ
        generator.add_test_result(Level2TestResult(
            test_name="ä¸€æ‹¬æ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
            test_class="TestLevel2DatabasePerformanceIntegration",
            test_method="test_bulk_operations_integration",
            status="passed",
            duration=5.2,
            warnings=["ä½œæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Š: 0.6ç§’ > 0.5ç§’"],
            performance_metrics={
                "response_time": 3.1,
                "creation_per_item": 0.6,
                "update_per_item": 0.2,
                "retrieval_per_item": 0.1
            }
        ))
        
        print(f"âœ… ãƒ†ã‚¹ãƒˆçµæœè¿½åŠ å®Œäº†: {len(generator.test_results)}ä»¶")
        
        # Step 3: ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        print("ğŸ“ˆ Step 3: ã‚µãƒãƒªãƒ¼ç”Ÿæˆ")
        summary = generator.generate_summary()
        
        assert summary.total_tests == 5
        assert summary.passed_tests == 3
        assert summary.failed_tests == 1
        assert summary.skipped_tests == 1
        assert summary.success_rate == 60.0  # 3/5 * 100
        
        print(f"âœ… ã‚µãƒãƒªãƒ¼ç”Ÿæˆå®Œäº†: æˆåŠŸç‡ {summary.success_rate}%")
        
        # Step 4: JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("ğŸ’¾ Step 4: JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        json_filepath = generator.save_json_report(summary, "test_level2_report.json")
        
        assert os.path.exists(json_filepath)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª
        with open(json_filepath, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        assert "summary" in json_data
        assert "test_results" in json_data
        assert "metadata" in json_data
        assert json_data["summary"]["total_tests"] == 5
        
        print(f"âœ… JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {json_filepath}")
        
        # Step 5: Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("ğŸ“ Step 5: Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        md_filepath = generator.save_markdown_report(summary, "test_level2_report.md")
        
        assert os.path.exists(md_filepath)
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª
        with open(md_filepath, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        assert "# Level 2 çµ±åˆãƒ†ã‚¹ãƒˆ ãƒ¬ãƒãƒ¼ãƒˆ" in md_content
        assert "ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚µãƒãƒªãƒ¼" in md_content
        assert "ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ" in md_content
        assert "ğŸ’¡ æ¨å¥¨äº‹é …" in md_content
        assert "æˆåŠŸç‡: 60.0%" in md_content
        
        print(f"âœ… Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {md_filepath}")
        
        # Step 6: ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹æ¤œè¨¼
        print("ğŸ” Step 6: ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹æ¤œè¨¼")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ç¢ºèª
        assert summary.performance_summary["total_performance_tests"] == 2
        assert summary.performance_summary["average_response_time"] > 0
        assert len(summary.performance_summary["performance_warnings"]) == 1
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªç¢ºèª
        assert "APIçµ±åˆ" in summary.test_categories
        assert "ã‚µãƒ¼ãƒ“ã‚¹çµ±åˆ" in summary.test_categories
        assert "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ" in summary.test_categories
        
        print("âœ… ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹æ¤œè¨¼å®Œäº†")
        
        print("âœ… Level 2çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½")
    
    @pytest.mark.integration
    def test_level2_report_performance_analysis(self):
        """
        ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«: Level 2 (Integration)
        å¯¾è±¡: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†ææ©Ÿèƒ½
        """
        print("\nğŸ§ª Level 2çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†ææ©Ÿèƒ½")
        
        # Step 1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœæº–å‚™
        print("ğŸ“Š Step 1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœæº–å‚™")
        generator = Level2TestReportGenerator("test_reports/level2_performance")
        
        # è¤‡æ•°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœã‚’è¿½åŠ 
        performance_tests = [
            {"name": "APIå¿œç­”æ™‚é–“ãƒ†ã‚¹ãƒˆ", "response_time": 0.8, "status": "passed"},
            {"name": "DBæ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", "response_time": 1.5, "status": "passed"},
            {"name": "ä¸€æ‹¬å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", "response_time": 3.2, "status": "passed", "warning": "å‡¦ç†æ™‚é–“è­¦å‘Š"},
            {"name": "åŒæ™‚æ¥ç¶šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", "response_time": 2.1, "status": "failed", "error": "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"},
        ]
        
        for i, test in enumerate(performance_tests):
            result = Level2TestResult(
                test_name=test["name"],
                test_class="TestLevel2PerformanceIntegration",
                test_method=f"test_performance_{i}",
                status=test["status"],
                duration=test["response_time"],
                performance_metrics={"response_time": test["response_time"]},
                error_message=test.get("error"),
                warnings=[test["warning"]] if test.get("warning") else []
            )
            generator.add_test_result(result)
        
        # Step 2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        print("ğŸ“ˆ Step 2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ")
        summary = generator.generate_summary()
        
        perf_summary = summary.performance_summary
        assert perf_summary["total_performance_tests"] == 4
        assert perf_summary["average_response_time"] == (0.8 + 1.5 + 3.2 + 2.1) / 4
        assert perf_summary["max_response_time"] == 3.2
        assert perf_summary["min_response_time"] == 0.8
        assert len(perf_summary["performance_warnings"]) == 1
        
        print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Œäº†: å¹³å‡ {perf_summary['average_response_time']:.2f}ç§’")
        
        # Step 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("ğŸ“ Step 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        md_content = generator.generate_markdown_report(summary)
        
        assert "ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ" in md_content
        assert f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {perf_summary['average_response_time']:.3f}ç§’" in md_content
        assert "âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Š" in md_content
        
        print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        
        print("âœ… Level 2çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†ææ©Ÿèƒ½")


if __name__ == "__main__":
    # å˜ä½“å®Ÿè¡Œç”¨
    pytest.main([__file__, "-v", "-s", "--tb=short", "-m", "integration"]) 